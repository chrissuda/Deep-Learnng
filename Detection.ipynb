{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.70s)\n",
      "creating index...\n",
      "index created!\n",
      "Val samples: 5000\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "                T.Resize((224,224)),\n",
    "                T.ToTensor()])\n",
    "\n",
    "\n",
    "coco_val=torchvision.datasets.CocoDetection(root=\"/home/chris/cnn/coco/val2017\", \n",
    "                                            annFile=\"/home/chris/cnn/coco/annotations/instances_val2017.json\", \n",
    "                                            transform=transform)\n",
    "#sampler=sampler.SubsetRandomSampler(range(200))\n",
    "loader_val = DataLoader(coco_val, batch_size=10, shuffle=True)\n",
    "print(\"Val samples:\",len(coco_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-753bef235b22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for imgs,labels in data_loader:\n",
    "    for i in range(len(imgs)):\n",
    "        bboxes = []\n",
    "        ids = []\n",
    "        img = imgs[i]\n",
    "        labels_ = labels[i]\n",
    "        for label in labels_:\n",
    "            bboxes.append([label['bbox'][0],\n",
    "            label['bbox'][1],\n",
    "            label['bbox'][0] + label['bbox'][2],\n",
    "            label['bbox'][1] + label['bbox'][3]\n",
    "            ])\n",
    "            ids.append(label['category_id'])\n",
    " \n",
    "        img = img.permute(1,2,0).numpy()\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "        for box ,id_ in zip(bboxes,ids):\n",
    "            x1 = int(box[0])\n",
    "            y1 = int(box[1])\n",
    "            x2 = int(box[2])\n",
    "            y2 = int(box[3])\n",
    "            cv2.rectangle(img,(x1,y1),(x2,y2),(0,0,255),thickness=2)\n",
    "            cv2.putText(img, text=str(id_), org=(x1 + 5, y1 + 5), fontFace=font, fontScale=1, \n",
    "                thickness=2, lineType=cv2.LINE_AA, color=(0, 255, 0))\n",
    "        cv2.imshow('test',img)\n",
    "        cv2.waitKey()\n",
    "        cv2.cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224])\n",
      "1\n",
      "segmentation : [[tensor([186.7200,  49.5600, 241.8600, 116.6800,  81.5500,  26.2700, 177.5200,\n",
      "        218.8900, 259.2900, 315.6900], dtype=torch.float64), tensor([122.4700, 274.3100, 222.3800, 221.0200, 212.5200, 163.7100, 272.2500,\n",
      "        325.7900, 286.8500, 133.3800], dtype=torch.float64), tensor([188.4000,  94.7500, 221.8800, 236.7000, 108.4200,  35.7100, 190.4700,\n",
      "        214.3900, 253.6300, 299.3800], dtype=torch.float64), tensor([130.3900, 284.8500, 228.1700, 222.2800, 204.8400, 155.6200, 243.4900,\n",
      "        335.5800, 315.1500, 151.6100], dtype=torch.float64), tensor([185.7600, 187.0100, 183.2200, 247.4300, 129.5300,  64.0000, 204.8500,\n",
      "        205.5800, 241.1800, 296.5000], dtype=torch.float64), tensor([137.3500, 306.3200, 224.3100, 214.0700, 199.0900, 138.7800, 217.6000,\n",
      "        338.3200, 343.4600, 198.6300], dtype=torch.float64), tensor([189.6000, 187.0100, 179.0300, 264.4900, 157.3500,  90.9500, 222.1100,\n",
      "        202.0500, 236.6500, 329.1300], dtype=torch.float64), tensor([167.8300, 310.8400, 220.4400, 221.0200, 193.3300, 130.6900, 204.6600,\n",
      "        340.2800, 376.3000, 222.6200], dtype=torch.float64), tensor([207.6000, 185.5100, 176.7700, 291.6500, 179.4200,  97.0100, 230.7400,\n",
      "        192.6500, 237.7800, 350.2400], dtype=torch.float64), tensor([163.2700, 356.7400, 216.5800, 217.8600, 189.4900, 125.9800, 190.2700,\n",
      "        333.4200, 405.7400, 239.8900], dtype=torch.float64), tensor([203.8800, 187.0100, 182.9000, 305.5500, 196.6900, 130.6900, 229.3000,\n",
      "        179.5300, 241.1800, 355.0300], dtype=torch.float64), tensor([158.6100, 365.7800, 211.7400, 167.9600, 187.5700, 106.4400, 174.4500,\n",
      "        314.2400, 415.9300, 289.7800], dtype=torch.float64), tensor([203.8800, 147.0900, 184.1800, 316.2800, 225.4700, 151.5800, 220.6700,\n",
      "        179.5300, 221.9300, 336.8000], dtype=torch.float64), tensor([153.2500, 370.3000, 209.4900, 157.8600, 196.2100,  92.9700, 174.4500,\n",
      "        311.1000, 415.9300, 404.9300], dtype=torch.float64), tensor([210.9100, 141.4400, 180.3200, 332.7100, 243.7000, 185.9400, 203.4100,\n",
      "        183.2500, 198.1500, 329.1300], dtype=torch.float64), tensor([151.5800, 371.0500, 208.8400, 155.3300, 204.8400,  82.8600, 175.8900,\n",
      "        310.9100, 413.6600, 414.5300], dtype=torch.float64), tensor([212.9200, 119.9800, 178.7100, 351.0200, 257.1400, 236.4600, 197.6600,\n",
      "        183.8400, 198.1500, 319.5300], dtype=torch.float64), tensor([150.2400, 365.7800, 206.2700, 168.5900, 215.4000,  78.1500, 180.2100,\n",
      "        308.5600, 396.6800, 420.2800], dtype=torch.float64), tensor([209.2400, 118.4700, 199.0100, 354.8100, 263.8500, 287.6600, 194.7800,\n",
      "        180.9000, 204.9400, 197.6700], dtype=torch.float64), tensor([135.5100, 368.4200, 197.8900, 190.7000, 220.2000,  83.5400, 191.7100,\n",
      "        304.4400, 369.5000, 422.2000], dtype=torch.float64), tensor([202.8800, 109.0600, 184.1800, 352.2900, 274.4100, 328.0800, 191.9100,\n",
      "        181.1000, 216.2700, 152.5700], dtype=torch.float64), tensor([128.8200, 368.7900, 187.9000, 213.4400, 225.9500,  91.6200, 194.5900,\n",
      "        304.2500, 293.6400, 421.2400], dtype=torch.float64), tensor([200.5400, 102.6600, 189.6600, 353.5500, 292.6400, 376.5900, 173.2100,\n",
      "        181.3000, 210.6100, 142.0100], dtype=torch.float64), tensor([119.7800, 358.6300, 176.3000, 218.4900, 230.7500, 104.4200, 194.5900,\n",
      "        300.5300, 281.1800, 370.3900], dtype=torch.float64)]] \n",
      "\n",
      "area : tensor([  869.7821, 11757.6196, 12150.0737, 12656.0081, 72405.3017, 42793.4287,\n",
      "        19252.1457,  1202.1250, 81350.5866, 52934.0281], dtype=torch.float64) \n",
      "\n",
      "iscrowd : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) \n",
      "\n",
      "image_id : tensor([523100, 446206,  90631, 196185, 117425, 504000, 451084, 359677, 535156,\n",
      "        167240]) \n",
      "\n",
      "category_id : tensor([44,  6,  5,  5, 67,  5,  1,  1, 22, 86]) \n",
      "\n",
      "id : tensor([  83647,  165939,  159679,  158368,  391052,  162508,  425291,  439308,\n",
      "         583861, 1155125]) \n",
      "\n",
      "boxes : [tensor([185.7600,  13.4100, 176.7700, 105.3100,   1.9200,  25.6000, 167.4600,\n",
      "        179.5300, 133.6100, 132.4200], dtype=torch.float64), tensor([119.7800, 273.5200, 115.7300, 155.3300, 187.5700,  43.1200, 121.2400,\n",
      "        292.1100,  50.2000, 116.1100], dtype=torch.float64), tensor([ 27.1600, 173.6000, 280.9500, 421.9400, 455.7400, 595.5400, 148.1300,\n",
      "         52.0900, 397.4300, 222.6100], dtype=torch.float64), tensor([ 48.0500,  97.5300, 112.4400, 120.6400, 232.1900, 173.1300, 257.4400,\n",
      "         48.3600, 388.3700, 306.0900], dtype=torch.float64)] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, progress=True, num_classes=91)\n",
    "x,y=next(iter(loader_val))\n",
    "print(x.shape)\n",
    "#images = list(image for image in images)\n",
    "print(len(y))\n",
    "for i in y:\n",
    "    for k,v in i.items():\n",
    "        #print(k,\":\",v,\"\\n\")\n",
    "        if(k==\"bbox\"):\n",
    "            i[\"boxes\"]=i.pop(k)\n",
    "    #print(\"\\n\\n\\n\")\n",
    "\n",
    "for i in y:\n",
    "    for k,v in i.items():\n",
    "        print(k,\":\",v,\"\\n\")\n",
    "\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform()\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign()\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 224, 224])\n",
      "1\n",
      "3\n",
      "[{'boxes': tensor([[2.3701e+00, 2.1115e+01, 2.2241e+02, 2.2235e+02],\n",
      "        [1.1079e-01, 1.3326e+02, 1.8805e+01, 2.1920e+02],\n",
      "        [0.0000e+00, 2.4229e+01, 2.2379e+02, 2.2284e+02],\n",
      "        [0.0000e+00, 1.6664e+01, 2.2269e+02, 2.2324e+02],\n",
      "        [0.0000e+00, 1.5787e+01, 2.2400e+02, 2.2400e+02]],\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([23, 20, 18, 88, 20]), 'scores': tensor([0.7830, 0.3583, 0.3268, 0.1616, 0.0821], grad_fn=<IndexBackward>)}, {'boxes': tensor([[192.2898,  42.0506, 198.9308,  59.3003],\n",
      "        [  3.0498,  10.8283, 184.0102, 213.4863],\n",
      "        [  8.6687,  94.1688,  15.9782, 103.1176],\n",
      "        [  5.5236,  17.7555, 187.3072, 206.3174],\n",
      "        [159.7656, 116.8083, 166.1217, 134.9307],\n",
      "        [183.1601,  88.5057, 219.4182, 115.5731],\n",
      "        [  8.6941,  92.7114,  15.1117, 103.4218]], grad_fn=<StackBackward>), 'labels': tensor([10,  7,  3,  6,  1,  7,  1]), 'scores': tensor([0.9153, 0.8656, 0.8184, 0.7002, 0.3664, 0.2989, 0.1865],\n",
      "       grad_fn=<IndexBackward>)}, {'boxes': tensor([[5.8399e+01, 1.1847e+02, 7.2024e+01, 1.5793e+02],\n",
      "        [1.3543e+02, 1.4174e+02, 1.3927e+02, 1.5129e+02],\n",
      "        [4.1346e-02, 1.0153e+02, 3.0274e+01, 1.1739e+02],\n",
      "        [4.0516e+01, 1.1661e+02, 5.0551e+01, 1.4677e+02],\n",
      "        [3.8216e+01, 1.2568e+02, 5.1017e+01, 1.5755e+02],\n",
      "        [6.6110e+00, 8.5502e+01, 1.1948e+02, 1.4400e+02],\n",
      "        [1.2062e+02, 1.1444e+02, 1.2332e+02, 1.2097e+02],\n",
      "        [4.2076e+01, 1.1660e+02, 5.0653e+01, 1.3406e+02],\n",
      "        [1.1105e+02, 1.0273e+02, 1.4914e+02, 1.1454e+02],\n",
      "        [6.8249e+01, 8.4070e+01, 1.6919e+02, 1.3841e+02],\n",
      "        [1.3958e+02, 1.4106e+02, 1.4495e+02, 1.5101e+02],\n",
      "        [4.0879e+00, 8.4375e+01, 1.6395e+02, 1.1675e+02],\n",
      "        [1.1331e+02, 1.0457e+02, 1.4746e+02, 1.1493e+02],\n",
      "        [9.3178e+01, 1.0042e+02, 1.5234e+02, 1.2025e+02],\n",
      "        [1.4476e+02, 1.4092e+02, 1.4872e+02, 1.5109e+02],\n",
      "        [1.2126e+02, 1.1268e+02, 1.7537e+02, 1.5012e+02],\n",
      "        [1.1618e+02, 1.0705e+02, 1.3814e+02, 1.1440e+02],\n",
      "        [1.2135e+02, 1.1292e+02, 1.7923e+02, 1.5087e+02],\n",
      "        [4.2922e+01, 1.1615e+02, 4.9524e+01, 1.2722e+02],\n",
      "        [1.5073e+02, 1.4148e+02, 1.5533e+02, 1.5449e+02],\n",
      "        [1.1978e+02, 1.1398e+02, 1.2471e+02, 1.2121e+02],\n",
      "        [6.8882e+01, 8.2692e+01, 1.6941e+02, 1.3704e+02],\n",
      "        [6.7467e+01, 8.8840e+01, 1.2623e+02, 1.4257e+02],\n",
      "        [1.0438e+02, 1.0181e+02, 1.5128e+02, 1.1693e+02],\n",
      "        [3.9969e+01, 8.7510e+01, 1.1249e+02, 1.4555e+02],\n",
      "        [1.1114e+02, 1.0976e+02, 1.2209e+02, 1.1503e+02],\n",
      "        [1.7923e+00, 8.5018e+01, 8.9607e+01, 1.2142e+02],\n",
      "        [1.6825e+02, 1.3750e+02, 1.7642e+02, 1.4736e+02],\n",
      "        [1.5035e+02, 1.4173e+02, 1.5481e+02, 1.5498e+02],\n",
      "        [1.1756e+02, 1.1355e+02, 1.8993e+02, 1.4949e+02],\n",
      "        [3.8052e+01, 1.2030e+02, 6.0786e+01, 1.5717e+02],\n",
      "        [4.2665e+01, 1.2666e+02, 5.1400e+01, 1.5022e+02],\n",
      "        [7.2091e+01, 9.1084e+01, 1.1605e+02, 1.3912e+02]],\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([ 1, 44,  5,  1, 19,  5,  1,  1,  5,  5, 44,  5,  8,  5, 44, 28,  8,  9,\n",
      "         1,  1,  1,  9,  5,  8,  9,  3,  5,  1, 44, 38, 19, 19,  9]), 'scores': tensor([0.9964, 0.9133, 0.9123, 0.9012, 0.8703, 0.8693, 0.7243, 0.6417, 0.5520,\n",
      "        0.3885, 0.3493, 0.3015, 0.2988, 0.2650, 0.2042, 0.2034, 0.1666, 0.1492,\n",
      "        0.1140, 0.1129, 0.1050, 0.1032, 0.0960, 0.0842, 0.0794, 0.0759, 0.0734,\n",
      "        0.0731, 0.0672, 0.0648, 0.0593, 0.0544, 0.0526],\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "\n",
      "\n",
      "[{'boxes': tensor([[2.3701e+00, 2.1115e+01, 2.2241e+02, 2.2235e+02],\n",
      "        [1.1079e-01, 1.3326e+02, 1.8805e+01, 2.1920e+02],\n",
      "        [0.0000e+00, 2.4229e+01, 2.2379e+02, 2.2284e+02],\n",
      "        [0.0000e+00, 1.6664e+01, 2.2269e+02, 2.2324e+02],\n",
      "        [0.0000e+00, 1.5787e+01, 2.2400e+02, 2.2400e+02]],\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([23, 20, 18, 88, 20]), 'scores': tensor([0.7830, 0.3583, 0.3268, 0.1616, 0.0821], grad_fn=<IndexBackward>)}, {'boxes': tensor([[192.2898,  42.0506, 198.9308,  59.3003],\n",
      "        [  3.0498,  10.8283, 184.0102, 213.4863],\n",
      "        [  8.6687,  94.1688,  15.9782, 103.1176],\n",
      "        [  5.5236,  17.7555, 187.3072, 206.3174],\n",
      "        [159.7656, 116.8083, 166.1217, 134.9307],\n",
      "        [183.1601,  88.5057, 219.4182, 115.5731],\n",
      "        [  8.6941,  92.7114,  15.1117, 103.4218]], grad_fn=<StackBackward>), 'labels': tensor([10,  7,  3,  6,  1,  7,  1]), 'scores': tensor([0.9153, 0.8656, 0.8184, 0.7002, 0.3664, 0.2989, 0.1865],\n",
      "       grad_fn=<IndexBackward>)}, {'boxes': tensor([[5.8399e+01, 1.1847e+02, 7.2024e+01, 1.5793e+02],\n",
      "        [1.3543e+02, 1.4174e+02, 1.3927e+02, 1.5129e+02],\n",
      "        [4.1346e-02, 1.0153e+02, 3.0274e+01, 1.1739e+02],\n",
      "        [4.0516e+01, 1.1661e+02, 5.0551e+01, 1.4677e+02],\n",
      "        [3.8216e+01, 1.2568e+02, 5.1017e+01, 1.5755e+02],\n",
      "        [6.6110e+00, 8.5502e+01, 1.1948e+02, 1.4400e+02],\n",
      "        [1.2062e+02, 1.1444e+02, 1.2332e+02, 1.2097e+02],\n",
      "        [4.2076e+01, 1.1660e+02, 5.0653e+01, 1.3406e+02],\n",
      "        [1.1105e+02, 1.0273e+02, 1.4914e+02, 1.1454e+02],\n",
      "        [6.8249e+01, 8.4070e+01, 1.6919e+02, 1.3841e+02],\n",
      "        [1.3958e+02, 1.4106e+02, 1.4495e+02, 1.5101e+02],\n",
      "        [4.0879e+00, 8.4375e+01, 1.6395e+02, 1.1675e+02],\n",
      "        [1.1331e+02, 1.0457e+02, 1.4746e+02, 1.1493e+02],\n",
      "        [9.3178e+01, 1.0042e+02, 1.5234e+02, 1.2025e+02],\n",
      "        [1.4476e+02, 1.4092e+02, 1.4872e+02, 1.5109e+02],\n",
      "        [1.2126e+02, 1.1268e+02, 1.7537e+02, 1.5012e+02],\n",
      "        [1.1618e+02, 1.0705e+02, 1.3814e+02, 1.1440e+02],\n",
      "        [1.2135e+02, 1.1292e+02, 1.7923e+02, 1.5087e+02],\n",
      "        [4.2922e+01, 1.1615e+02, 4.9524e+01, 1.2722e+02],\n",
      "        [1.5073e+02, 1.4148e+02, 1.5533e+02, 1.5449e+02],\n",
      "        [1.1978e+02, 1.1398e+02, 1.2471e+02, 1.2121e+02],\n",
      "        [6.8882e+01, 8.2692e+01, 1.6941e+02, 1.3704e+02],\n",
      "        [6.7467e+01, 8.8840e+01, 1.2623e+02, 1.4257e+02],\n",
      "        [1.0438e+02, 1.0181e+02, 1.5128e+02, 1.1693e+02],\n",
      "        [3.9969e+01, 8.7510e+01, 1.1249e+02, 1.4555e+02],\n",
      "        [1.1114e+02, 1.0976e+02, 1.2209e+02, 1.1503e+02],\n",
      "        [1.7923e+00, 8.5018e+01, 8.9607e+01, 1.2142e+02],\n",
      "        [1.6825e+02, 1.3750e+02, 1.7642e+02, 1.4736e+02],\n",
      "        [1.5035e+02, 1.4173e+02, 1.5481e+02, 1.5498e+02],\n",
      "        [1.1756e+02, 1.1355e+02, 1.8993e+02, 1.4949e+02],\n",
      "        [3.8052e+01, 1.2030e+02, 6.0786e+01, 1.5717e+02],\n",
      "        [4.2665e+01, 1.2666e+02, 5.1400e+01, 1.5022e+02],\n",
      "        [7.2091e+01, 9.1084e+01, 1.1605e+02, 1.3912e+02]],\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([ 1, 44,  5,  1, 19,  5,  1,  1,  5,  5, 44,  5,  8,  5, 44, 28,  8,  9,\n",
      "         1,  1,  1,  9,  5,  8,  9,  3,  5,  1, 44, 38, 19, 19,  9]), 'scores': tensor([0.9964, 0.9133, 0.9123, 0.9012, 0.8703, 0.8693, 0.7243, 0.6417, 0.5520,\n",
      "        0.3885, 0.3493, 0.3015, 0.2988, 0.2650, 0.2042, 0.2034, 0.1666, 0.1492,\n",
      "        0.1140, 0.1129, 0.1050, 0.1032, 0.0960, 0.0842, 0.0794, 0.0759, 0.0734,\n",
      "        0.0731, 0.0672, 0.0648, 0.0593, 0.0544, 0.0526],\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "\n",
      "\n",
      "[{'boxes': tensor([[2.3701e+00, 2.1115e+01, 2.2241e+02, 2.2235e+02],\n",
      "        [1.1079e-01, 1.3326e+02, 1.8805e+01, 2.1920e+02],\n",
      "        [0.0000e+00, 2.4229e+01, 2.2379e+02, 2.2284e+02],\n",
      "        [0.0000e+00, 1.6664e+01, 2.2269e+02, 2.2324e+02],\n",
      "        [0.0000e+00, 1.5787e+01, 2.2400e+02, 2.2400e+02]],\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([23, 20, 18, 88, 20]), 'scores': tensor([0.7830, 0.3583, 0.3268, 0.1616, 0.0821], grad_fn=<IndexBackward>)}, {'boxes': tensor([[192.2898,  42.0506, 198.9308,  59.3003],\n",
      "        [  3.0498,  10.8283, 184.0102, 213.4863],\n",
      "        [  8.6687,  94.1688,  15.9782, 103.1176],\n",
      "        [  5.5236,  17.7555, 187.3072, 206.3174],\n",
      "        [159.7656, 116.8083, 166.1217, 134.9307],\n",
      "        [183.1601,  88.5057, 219.4182, 115.5731],\n",
      "        [  8.6941,  92.7114,  15.1117, 103.4218]], grad_fn=<StackBackward>), 'labels': tensor([10,  7,  3,  6,  1,  7,  1]), 'scores': tensor([0.9153, 0.8656, 0.8184, 0.7002, 0.3664, 0.2989, 0.1865],\n",
      "       grad_fn=<IndexBackward>)}, {'boxes': tensor([[5.8399e+01, 1.1847e+02, 7.2024e+01, 1.5793e+02],\n",
      "        [1.3543e+02, 1.4174e+02, 1.3927e+02, 1.5129e+02],\n",
      "        [4.1346e-02, 1.0153e+02, 3.0274e+01, 1.1739e+02],\n",
      "        [4.0516e+01, 1.1661e+02, 5.0551e+01, 1.4677e+02],\n",
      "        [3.8216e+01, 1.2568e+02, 5.1017e+01, 1.5755e+02],\n",
      "        [6.6110e+00, 8.5502e+01, 1.1948e+02, 1.4400e+02],\n",
      "        [1.2062e+02, 1.1444e+02, 1.2332e+02, 1.2097e+02],\n",
      "        [4.2076e+01, 1.1660e+02, 5.0653e+01, 1.3406e+02],\n",
      "        [1.1105e+02, 1.0273e+02, 1.4914e+02, 1.1454e+02],\n",
      "        [6.8249e+01, 8.4070e+01, 1.6919e+02, 1.3841e+02],\n",
      "        [1.3958e+02, 1.4106e+02, 1.4495e+02, 1.5101e+02],\n",
      "        [4.0879e+00, 8.4375e+01, 1.6395e+02, 1.1675e+02],\n",
      "        [1.1331e+02, 1.0457e+02, 1.4746e+02, 1.1493e+02],\n",
      "        [9.3178e+01, 1.0042e+02, 1.5234e+02, 1.2025e+02],\n",
      "        [1.4476e+02, 1.4092e+02, 1.4872e+02, 1.5109e+02],\n",
      "        [1.2126e+02, 1.1268e+02, 1.7537e+02, 1.5012e+02],\n",
      "        [1.1618e+02, 1.0705e+02, 1.3814e+02, 1.1440e+02],\n",
      "        [1.2135e+02, 1.1292e+02, 1.7923e+02, 1.5087e+02],\n",
      "        [4.2922e+01, 1.1615e+02, 4.9524e+01, 1.2722e+02],\n",
      "        [1.5073e+02, 1.4148e+02, 1.5533e+02, 1.5449e+02],\n",
      "        [1.1978e+02, 1.1398e+02, 1.2471e+02, 1.2121e+02],\n",
      "        [6.8882e+01, 8.2692e+01, 1.6941e+02, 1.3704e+02],\n",
      "        [6.7467e+01, 8.8840e+01, 1.2623e+02, 1.4257e+02],\n",
      "        [1.0438e+02, 1.0181e+02, 1.5128e+02, 1.1693e+02],\n",
      "        [3.9969e+01, 8.7510e+01, 1.1249e+02, 1.4555e+02],\n",
      "        [1.1114e+02, 1.0976e+02, 1.2209e+02, 1.1503e+02],\n",
      "        [1.7923e+00, 8.5018e+01, 8.9607e+01, 1.2142e+02],\n",
      "        [1.6825e+02, 1.3750e+02, 1.7642e+02, 1.4736e+02],\n",
      "        [1.5035e+02, 1.4173e+02, 1.5481e+02, 1.5498e+02],\n",
      "        [1.1756e+02, 1.1355e+02, 1.8993e+02, 1.4949e+02],\n",
      "        [3.8052e+01, 1.2030e+02, 6.0786e+01, 1.5717e+02],\n",
      "        [4.2665e+01, 1.2666e+02, 5.1400e+01, 1.5022e+02],\n",
      "        [7.2091e+01, 9.1084e+01, 1.1605e+02, 1.3912e+02]],\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([ 1, 44,  5,  1, 19,  5,  1,  1,  5,  5, 44,  5,  8,  5, 44, 28,  8,  9,\n",
      "         1,  1,  1,  9,  5,  8,  9,  3,  5,  1, 44, 38, 19, 19,  9]), 'scores': tensor([0.9964, 0.9133, 0.9123, 0.9012, 0.8703, 0.8693, 0.7243, 0.6417, 0.5520,\n",
      "        0.3885, 0.3493, 0.3015, 0.2988, 0.2650, 0.2042, 0.2034, 0.1666, 0.1492,\n",
      "        0.1140, 0.1129, 0.1050, 0.1032, 0.0960, 0.0842, 0.0794, 0.0759, 0.0734,\n",
      "        0.0731, 0.0672, 0.0648, 0.0593, 0.0544, 0.0526],\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "images is expected to be a list of 3d tensors of shape [C, H, W], got torch.Size([224, 224])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3b203b9b17cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 raise ValueError(\"images is expected to be a list of 3d tensors \"\n\u001b[0;32m---> 45\u001b[0;31m                                  \"of shape [C, H, W], got {}\".format(image.shape))\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: images is expected to be a list of 3d tensors of shape [C, H, W], got torch.Size([224, 224])"
     ]
    }
   ],
   "source": [
    "# for a in y:\n",
    "#     a[\"boxes\"]=a.pop(\"bbox\")\n",
    "print(x.shape)\n",
    "print(len(y))\n",
    "model.eval()\n",
    "loss=model(x)\n",
    "print(len(loss))\n",
    "for l in loss:\n",
    "    print(loss)\n",
    "    print(\"\\n\")\n",
    "print('\\n\\n')\n",
    "image=x[0,:,:,:]\n",
    "print(image.shape)\n",
    "print(model(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6ac77c28697b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "targets={}\n",
    "for k,v in y[0].items():\n",
    "    targets[k]=v\n",
    "print(type(targets))\n",
    "model.train()\n",
    "loss=model(x,targets)\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "x=x.permute(0,2,3,1)\n",
    "print(x[0].shape)\n",
    "plt.imshow(x[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y))\n",
    "print(len(y))\n",
    "print(y[0])\n",
    "y[0][\"boxes\"]=y[0].pop(\"bbox\")\n",
    "for key,value in y[0].items():\n",
    "    print(key,\":\",value)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to default device\n",
    "criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy)\n",
    "\n",
    "# Epochs\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    # Decay learning rate at particular epochs\n",
    "    if epoch in decay_lr_at:\n",
    "        adjust_learning_rate(optimizer, decay_lr_to)\n",
    "\n",
    "    optimizer=torch.optim.Adam();\n",
    "    # One epoch's training\n",
    "    print(\"ok\")\n",
    "    train(train_loader=val_loader,\n",
    "          model=model,\n",
    "          criterion=criterion,\n",
    "          optimizer=optimizer,\n",
    "          epoch=epoch)\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param model: model\n",
    "    :param criterion: MultiBox loss\n",
    "    :param optimizer: optimizer\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "    model.train()  # training mode enables dropout\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to default device\n",
    "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
    "        boxes = [b.to(device) for b in boxes]\n",
    "        labels = [l.to(device) for l in labels]\n",
    "\n",
    "        # Forward prop.\n",
    "        predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
    "\n",
    "        # Backward prop.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer, grad_clip)\n",
    "\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                  batch_time=batch_time,\n",
    "                                                                  data_time=data_time, loss=losses))\n",
    "    del predicted_locs, predicted_scores, images, boxes, labels  # free some memory since their histories may be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
